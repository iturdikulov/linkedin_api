import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.http import Request
from scrapy.http import FormRequest
import json
from urllib.parse import urlparse, parse_qs
import urllib
import re
from bs4 import BeautifulSoup
import time
from html.parser import HTMLParser
import csv
import uuid
from datetime import datetime
import argparse
from scrapy import signals
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

from scrapy.signalmanager import dispatcher


class CustomItem(scrapy.Item):
    title = scrapy.Field()
    companyCounty = scrapy.Field()
    telephone = scrapy.Field()
    email = scrapy.Field()
    phones = scrapy.Field()
    websites = scrapy.Field()
    positions = scrapy.Field()
    educations = scrapy.Field()
    references = scrapy.Field()
    languages = scrapy.Field()
    tags = scrapy.Field()
    entityUrn = scrapy.Field()
    extractEmailAddress = scrapy.Field()


class LinkedinLeadItem(CustomItem):
    memberId = scrapy.Field()
    inCrm = scrapy.Field()
    firstname = scrapy.Field()
    lastname = scrapy.Field()
    fullname = scrapy.Field()
    degree = scrapy.Field()
    canSendInMail = scrapy.Field()
    headline = scrapy.Field()
    picture = scrapy.Field()
    profileLink = scrapy.Field()
    profileLinkSN = scrapy.Field()
    location = scrapy.Field()
    position = scrapy.Field()
    companyId = scrapy.Field()
    companyName = scrapy.Field()
    companyType = scrapy.Field()
    companyIndustry = scrapy.Field()
    companyDescription = scrapy.Field()
    companyWebsite = scrapy.Field()
    companyStaffCount = scrapy.Field()
    companyCountry = scrapy.Field()
    companyGeographicArea = scrapy.Field()
    companyCity = scrapy.Field()
    companyPostalCode = scrapy.Field()
    companyLine2 = scrapy.Field()
    companyLine1 = scrapy.Field()
    companyFounded = scrapy.Field()
    companyFollowerCount = scrapy.Field()
    companyEmails = scrapy.Field()
    companyLink = scrapy.Field()
    companyLinkSN = scrapy.Field()
    companySlug = scrapy.Field()
    publicIdentifier = scrapy.Field()
    spiderName = scrapy.Field()


class LoginSpider(scrapy.Spider):
    name = 'jiju'
    start_urls = ['https://www.linkedin.com/uas/login']
    clientPageInstance = None
    start_url = 'https://www.linkedin.com/'
    linkedin_login_url = 'https://www.linkedin.com/uas/login'
    sales_api_tags_url = 'https://www.linkedin.com/sales-api/salesApiTags?q=currentSeatHolder&tagType=LEAD_OR_ACCOUNT&isActiveOnly=true'
    profile_view_url = 'https://www.linkedin.com/voyager/api/identity/profiles/%s/profileView'
    company_additional_infos_url = 'https://www.linkedin.com/voyager/api/organization/companies?decoration=%28adsRule%2CaffiliatedCompaniesWithEmployeesRollup%2CaffiliatedCompaniesWithJobsRollup%2CarticlePermalinkForTopCompanies%2CautoGenerated%2CbackgroundCoverImage%2Cclaimable%2CclaimableByViewer%2CcompanyEmployeesSearchPageUrl%2CcompanyPageUrl%2CcoverPhoto%2CdataVersion%2Cdescription%2CentityUrn%2CfollowingInfo%2CfoundedOn%2Cheadquarter%2CjobSearchPageUrl%2ClcpTreatment%2Clogo%2Cname%2Ctype%2CoverviewPhoto%2CpaidCompany%2CpartnerCompanyUrl%2CpartnerLogo%2Cpermissions%2CrankForTopCompanies%2CrecentNewsAvailable%2CsalesNavigatorCompanyUrl%2Cschool%2Cshowcase%2CstaffCount%2CstaffCountRange%2CstaffingCompany%2CuniversalName%2Curl%2CviewerConnectedToAdministrator%2CviewerEmployee%2CviewerPendingAdministrator%2CcompanyIndustries*%2Cindustries%2Cspecialities%2CacquirerCompany~%28entityUrn%2Clogo%2Cname%2Cindustries%2CfollowingInfo%2Curl%2CpaidCompany%29%2CaffiliatedCompanies*~%28entityUrn%2Clogo%2Cname%2Cindustries%2CfollowingInfo%2Curl%2CpaidCompany%29%2Cgroups*~%28entityUrn%2ClargeLogo%2CgroupName%2CmemberCount%2CwebsiteUrl%2Curl%29%2CshowcasePages*~%28entityUrn%2Clogo%2Cname%2Cindustries%2CfollowingInfo%2Curl%2Cdescription%29%29&q=universalName&universalName={}'
    profile_contact_infos_url = 'https://www.linkedin.com/voyager/api/identity/profiles/%s/profileContactInfo'

    sales_tags = {}
    csrfToken = None
    LINKEDIN_LEADS_LIMIT = 5
    captcha_data_in = {
        "key": '0ed680416985d3a31047cbb500c2d0fd',
        "method": "userrecaptcha",
        "googlekey": "",
        "pageurl": "https://www.linkedin.com/",
        "json": "1"
    }

    captcha_data_out = {
        "key": '0ed680416985d3a31047cbb500c2d0fd',
        "action": "get",
        "json": "1",
        'id': ''
    }
    page = 1
    max_pages = 2
    max_sales_leads = 40

    leads = []
    leads_users = {}

    custom_settings = {
        'BOT_NAME': 'linkedin_sales_navigator',
        'SPIDER_MODULES': ['linkedin_sales_navigator.spiders'],
        'NEWSPIDER_MODULE': 'linkedin_sales_navigator.spiders',
        'LINKEDIN_LEADS_LIMIT': 5,
        'FEED_EXPORT_ENCODING': 'utf-8',
        'PROXIES': [],
        'LOG_LEVEL': 'DEBUG',
        'DOWNLOAD_DELAY': 3.5,
        'CONCURRENT_REQUESTS': 1,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
        'CONCURRENT_REQUESTS_PER_IP': 1,
        'RETRY_TIMES': 2,
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1,
        'URLLENGTH_LIMIT': 1000000,
        'RETRY_HTTP_CODES': [500, 503, 504, 400, 401, 404, 403, 408, 429],
        'ROBOTSTXT_OBEY': False,
        'USER_AGENTS': [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:71.0) Gecko/20100101 Firefox/71.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.79 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:70.0) Gecko/20100101 Firefox/70.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36',
            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; rv:68.0) Gecko/20100101 Firefox/68.0',
            'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:70.0) Gecko/20100101 Firefox/70.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.18362',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Safari/605.1.15',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.4 Safari/605.1.15',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36',
            'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36',
            'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0',
            'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',
            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36',
            'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:71.0) Gecko/20100101 Firefox/71.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:71.0) Gecko/20100101 Firefox/71.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/18.17763',
            'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 YaBrowser/19.12.0.358 Yowser/2.5 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:70.0) Gecko/20100101 Firefox/70.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36',
            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.79 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:70.0) Gecko/20100101 Firefox/70.0',
        ],
        'DOWNLOADER_MIDDLEWARES': {
            'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
            'middlewares.RandomUserAgent': 400,
            'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,
            'middlewares.CustomProxyMiddleware': 350,
            'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 400,
        },
        # 'FEED_EXPORTERS': {
        #     'csv': 'linkedin_sales_navigator.exporters.CsvItemExporter',
        # },
    }

    htmlparser = HTMLParser()

    def parse(self, response):
        print(self.settings.get('LINKEDIN_SESSION_KEY'))

        formXpath = '//form[@id="login"]|//form[contains(@class, "login")]'
        formSelector = response.xpath(formXpath)
        if not formSelector:
            if 'login' not in response.url:
                return scrapy.Request(self.linkedin_login_url, callback=self.parse_start_url)
            raise ValueError('LinkedIn form login not found.')

        if response.meta.get('captcha_answer'):
            meta = {'captcha_answer': response.meta.get('captcha_answer')}
        else:
            meta = {}

        return scrapy.FormRequest.from_response(response, meta=meta, formxpath=formXpath,
                                                formdata={'session_key': self.settings.get('LINKEDIN_SESSION_KEY'),
                                                          'session_password': self.settings.get('LINKEDIN_SESSION_PASSWORD')},
                                                callback=self.process_login)

    def process_login(self, response):
        captcha_in_url = "http://x-captcha2.ru/in.php"
        captcha_out_url = "http://x-captcha2.ru/res.php"

        if not response.meta.get('force_login'):
            if response.meta.get('captcha_out'):
                data = response.body_as_unicode()
                data = json.loads(data)
                if data['status'] == 1:
                    captcha_answer = data['request']
                    print('Found captcha answer!')

                    return Request('https://www.linkedin.com/uas/login',
                                   dont_filter=True,
                                   callback=self.parse,
                                   meta={'captcha_answer': captcha_answer})

                else:
                    return Request(self.captcha_data_out,
                                       dont_filter=True,
                                       callback=self.process_login,
                                       meta={'captcha_out': True})

            elif response.meta.get('captcha_in'):
                data = response.body_as_unicode()
                data = json.loads(data)

                if data['status'] == 1:
                    print('Waiting for captcha answer')
                    self.captcha_data_out['id'] = data['request']
                    self.captcha_data_out = f'{captcha_out_url}?{urllib.parse.urlencode(self.captcha_data_out)}'

                    return Request(self.captcha_data_out,
                                       callback=self.process_login,
                                       meta={'captcha_out': True})
                else:
                    time.sleep(4)
                    print('Waiting for captcha in...', data)
                    return FormRequest(captcha_in_url,
                                      formdata=self.captcha_data_in,
                                      dont_filter=True,
                                      callback=self.process_login,
                                      meta={'captcha_in': True})

            if 'checkpoint/challenge/' in response.url:
                soup = BeautifulSoup(response.body.decode("utf-8"), 'html.parser')
                inputs = soup.find_all('input', {'type': 'hidden'})
                results = {}

                # get captcha key
                for txt_input in inputs:
                    name = txt_input.get('name')
                    value = txt_input.get('value')
                    if name and value:
                        results[name] = value

                if response.meta.get('captcha_answer'):
                    results['captchaUserResponseToken'] = response.meta.get('captcha_answer')
                    print(results, 'Found captcha_answer!')

                    formXpath = '//form[@id="captcha-challenge"]|//form[contains(@class, "hidden")]'
                    formSelector = response.xpath(formXpath)
                    if not formSelector:
                        raise ValueError('Captcha form not found.')

                    return scrapy.FormRequest.from_response(response,
                                                            meta={'force_login': True},
                                                            formxpath=formXpath,
                                                            formdata=results,
                                                            callback=self.process_login)

                elif results.get('captchaSiteKey'):
                    print('Captcha key found, solving...')
                    self.captcha_data_in['googlekey'] = results.get('captchaSiteKey')

                    captcha_answer = None
                    #captcha_request_1 = requests.post(captcha_in_url, params=captcha_data_in).json()

                    return FormRequest(captcha_in_url,
                                  formdata=self.captcha_data_in,
                                  callback=self.process_login,
                                  meta={'captcha_in': True})

                else:
                    print('PIN code required?')
                    print(results)
                    return

        csrfToken = re.search('(?i)JSESSIONID="?(ajax:\\d+?)"?;', response.request.headers.getlist('Cookie')[0].decode()).groups()
        loginFormExist = len(response.xpath('//form[@id="login"]|//form[contains(@class, "login")]'))
        if not csrfToken or loginFormExist:
            raise ValueError('Cannot login to LinkedIn, no CSRF Token found.')
        self.csrfToken = ('').join(csrfToken).strip()
        self.logger.debug('csrfToken %s', self.csrfToken)
        print('Successfully login to LinkedIn!')

        if self.settings.get('SEARCH_QUERY_URL').startswith('https://www.linkedin.com/sales/search/people?'):
            return self.after_login(response)
        elif self.settings.get('SEARCH_QUERY_URL').startswith('https://www.linkedin.com/search/results/people/?'):
            return self.normal_search(response)

    def normal_search(self, response):
        return scrapy.Request(url=self.settings.get('SEARCH_QUERY_URL'),
                              callback=self.normal_search_parse)

    def normal_search_parse(self, response):
        search_hits = response.xpath("//code//text()").extract()
        jsons = []
        print('Found search hits: ', len(search_hits))
        users = {}
        tmp_users = {}
        pagination = {}

        for item in search_hits:
            try:
                data = json.loads(item)
                data_type = data.get('data', {}).get('$type')
                jsons.append(data)

                paging = data.get('data', {}).get('paging')
                metadata = data.get('data', {}).get('metadata')

                if not pagination and \
                        metadata and \
                        metadata.get('origin') and \
                        'total' in paging:

                    pagination = paging

                if data_type == 'com.linkedin.restli.common.CollectionResponse':
                    elements = data.get('data', {}).get('elements')
                    for sub_element in elements:
                        sub_elements = sub_element.get('elements')
                        for item in sub_elements:
                            user_public_id = item.get('publicIdentifier')
                            users.setdefault(user_public_id, {})
                            users[user_public_id].update(item)

                mini_profiles = data.get('included')
                if mini_profiles and isinstance(mini_profiles, list):
                    for item in mini_profiles:
                        type = item.get('$type')
                        if type == 'com.linkedin.voyager.identity.shared.MiniProfile':
                            user_public_id = item.get('publicIdentifier')
                            tmp_users.setdefault(user_public_id, {})
                            tmp_users[user_public_id].update(item)

            except Exception as e:
                print(str(e))
                print('Failed parse item...')

        for key, user in tmp_users.items():
            if key in users:
                users[key].update(user)

        self.leads_users.update(users)

        if pagination and pagination.get('total') and self.page < self.max_pages:
            self.page += 1

            url = response.url

            if '&page=' not in url:
                url = url + '&page=' + str(self.page)
            else:
                url = url.replace('page=%s' % str(self.page - 1),
                                  'page=%s' % str(self.page))

            # wait some time
            yield scrapy.Request(url=url, callback=self.normal_search_parse)
        elif self.page > self.max_pages:
            return

        for public_id, lead in self.leads_users.items():
            i = LinkedinLeadItem()
            i['spiderName'] = 'normal_search'
            i['publicIdentifier'] = public_id
            i['extractEmailAddress'] = False  # TODO add extraction!
            i['firstname'] = lead.get('firstName')
            i['lastname'] = lead.get('lastName')
            i['fullname'] = lead.get('title', {}).get('text')
            degree = lead.get('secondaryTitle', {}).get('text')
            degree_num = -1
            if degree:
                degree_num = ''.join([n for n in degree if n.isdigit()])
                if degree_num.isdigit():
                    degree_num = int(degree_num)


            i['degree'] = degree_num
            i['canSendInMail'] = -1
            i['location'] = lead.get('subline', {}).get('text')
            i['inCrm'] = -1
            i['tags'] = ""
            entityUrn = None
            if 'entityUrn' in lead:
                entityUrn = ''.join(re.findall(r'urn:li:fs_miniProfile:(.*)', lead['entityUrn']))
                if entityUrn:
                    i['profileLinkSN'] = ''
                    i['profileLink'] = 'https://www.linkedin.com/profile/view/?id=%s' % entityUrn
                    i['entityUrn'] = entityUrn

            i['position'] = lead.get('headline', {}).get('text')

            snippet_text = lead.get('snippetText', {})
            if snippet_text and snippet_text.get('text'):
                company_name = re.findall(r'at(.*)?', snippet_text.get('text'))
                if len(company_name) == 1:
                    i['companyName'] = company_name[0].strip()

            if lead.get('picture', {}):
                pictures = lead.get('picture', {}).get('artifacts', [])
                if pictures:
                    for image in pictures:
                        if image['width'] == 400:
                            i['picture'] = '%s%s' % (lead['picture']['rootUrl'], image['fileIdentifyingUrlPathSegment'])
                            break

            #self.leads.append(i.__dict__['_values'])
            yield self.getPositionEducationForLeadsRequest(i)

    def after_login(self, response):
        # with open('test.html', 'w') as f:
        #     f.write(response.body.decode())

        # return scrapy.Request('https://www.linkedin.com/sales/search',
        #                       dont_filter=True,
        #                       callback=self.prepare_sales_page if 'leads' not in response.meta
        #                       else self.parseLeads)
        return scrapy.Request(url='https://www.linkedin.com/sales/search',
                              callback=self.prepare_sales_page)

    def prepare_sales_page(self, response):
        # print('URL:', response.url)
        # print('Meta:', response.meta)
        parsedURL = urlparse(response.url)
        qsDict = parse_qs(parsedURL.query)

        clientPageInstance = ('').join(response.xpath('//code[@id="clientPageInstance"]//text()').extract()).strip()
        if clientPageInstance:
            self.clientPageInstance = clientPageInstance

        csrfToken = re.search('(?i)JSESSIONID="?(ajax:\\d+?)"?;', response.request.headers.getlist('Cookie')[0].decode()).groups()

        if csrfToken:
            self.csrfToken = csrfToken

        if 'redirect' in qsDict:
            redirect = qsDict['redirect'][0]
        elif 'session_redirect' in qsDict:
            qsDict['session_redirect'][0]

        if 'salesApiAgnosticAuthentication' not in response.url:
            if 'findLicensesByCurrentMember' not in response.url:

                return scrapy.Request('https://www.linkedin.com/sales-api/salesApiIdentity?q=findLicensesByCurrentMember',
                                      headers={
                                          'dnt': '1',
                                          'accept-encoding': 'gzip, deflate, br',
                                          'x-li-lang': 'en_US',
                                          'accept-language': 'en-US,en;q=0.9',
                                          'x-requested-with': 'XMLHttpRequest',
                                          'pragma': 'no-cache',
                                          'accept': '*/*',
                                          'cache-control': 'no-cache',
                                          'x-restli-protocol-version': '2.0.0',
                                          'authority': 'www.linkedin.com',
                                          'referer': 'https://www.linkedin.com/sales/login',
                                          'Csrf-Token': self.csrfToken
                                      },
                                      meta={'redirect': redirect},
                                      callback=self.prepareSalesPage)


        raise ValueError('prepare_sales_page some error?')

    def prepareSalesPage(self, response):
        if 'Location' in response.headers:
            locationUrl = ('').join(response.headers['Location'])
            if 'checkpoint/enterprise/login' in locationUrl:
                return scrapy.Request(locationUrl, meta={'checkpoint': 1},
                                      dont_filter=True,
                                      callback=self.prepareSalesPage)

        if 'checkpoint' in response.meta and response.meta['checkpoint']:
            parsedURL = urlparse(response.url)
            parsedUrlQs = parse_qs(parsedURL.query)
            self.logger.debug('parsedUrlQs %s', parsedUrlQs)
            salesApiEnterpriseAuthenticationUrl = 'https://www.linkedin.com/sales-api/salesApiEnterpriseAuthentication?accountId=%s&appInstanceId=%s&budgetGroupId=%s&licenseType=%s&viewerDeviceType=DESKTOP'
            salesApiEnterpriseAuthenticationUrl = salesApiEnterpriseAuthenticationUrl % (parsedUrlQs['accountId'][0], parsedUrlQs['appInstanceId'][0], parsedUrlQs['budgetGroupId'][0], parsedUrlQs['licenseType'][0])


            byte_headers = {'Csrf-Token': self.csrfToken,
                            'X-Restli-Protocol-Version': '2.0.0',
                            'X-Requested-With': 'XMLHttpRequest'}

            headers = {y: byte_headers.get(y).decode() for y in byte_headers.keys()}

            return scrapy.Request(salesApiEnterpriseAuthenticationUrl,
                                  headers=headers,
                                  meta={'leads': 1},
                                  dont_filter=True,
                                  callback=self.prepareSalesPage)

        if 'redirect' in response.url or 'findLicensesByCurrentMember' in response.url:
            if 'salesApiAgnosticAuthentication' not in response.url:
                parsedURL = urlparse(response.url)
                qsDict = parse_qs(parsedURL.query)
                redirect = '/sales/search'

                if 'redirect' in response.meta:
                    redirect = response.meta['redirect']
                else:
                    if 'redirect' in qsDict:
                        redirect = qsDict['redirect'][0]
                    elif 'session_redirect' in qsDict:
                        qsDict['session_redirect'][0]

                redirect = urllib.parse.urlencode({'redirect': redirect})
                data = response.body_as_unicode()
                data = json.loads(data)
                print('Found data: ', data)

                if 'elements' in data and len(data['elements']):
                    element = data['elements'][0]
                    contractData = {'viewerDeviceType': 'DESKTOP',
                                    'name': element['name'],
                                    'identity': {'agnosticIdentity': element['agnosticIdentity'],
                                                 'name': element['name']}}

                    return scrapy.Request('https://www.linkedin.com/sales-api/salesApiAgnosticAuthentication?%s' % (redirect,),
                                          method='POST',
                                          headers={'Csrf-Token': self.csrfToken,
                                                   'X-Restli-Protocol-Version': '2.0.0',
                                                   'X-Requested-With': 'XMLHttpRequest',
                                                   'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
                                                   'X-Li-Page-Instance': self.clientPageInstance,
                                                   'X-Li-Lang': 'en_US',
                                                   'Referer': 'https://www.linkedin.com/sales/contract-chooser?redirect=%2Fsales%2Fsearch'
                                                   },
                                          body=json.dumps(contractData),
                                          callback=self.prepareSalesPage)


        return scrapy.Request('https://www.linkedin.com/sales-api/salesApiTags?q=currentSeatHolder&tagType=LEAD_OR_ACCOUNT&isActiveOnly=true',
       headers={'Csrf-Token': self.csrfToken,
                'X-Restli-Protocol-Version': '2.0.0',
                'X-Requested-With': 'XMLHttpRequest',
                'X-Li-Page-Instance': self.clientPageInstance,
                'Referer': ''},
                  meta={'handle_httpstatus_list': [
            400, 404, 403, 429]}, callback=self.parseSalesTags)

    def parseSalesTags(self, response):
        data = json.loads(response.body)
        if 'elements' in data and data['elements']:
            for element in data['elements']:
                if 'entityUrn' in element and element['entityUrn']:
                    self.sales_tags[element['entityUrn']] = element['value']

        return scrapy.Request(self.settings.get('SEARCH_QUERY_URL'),
                              meta={'retry_parsing_leads': 1},
                              dont_filter=True,
                              callback=self.parseLeads)

    def parseLeads(self, response):
        self.logger.info('parseLeads headers %s', response.request.headers)
        self.logger.info(response.url)
        leadsJson = ('').join(response.xpath('//code[@id="streamed-content-content"]/comment()').extract()).strip('<!--').strip('-->').strip()
        salesNavV2Request = None

        if not leadsJson:
            requestJson = ('').join(response.xpath("(//code[contains(., '/sales-api/salesApiPeopleSearch')]//text())[1]").extract()).strip()
            primaryIdentityJson = ('').join(response.xpath('//code[contains(., \'"primaryIdentity"\')]//text()').extract()).strip()
            clientPageInstance = ('').join(response.xpath('//code[@id="clientPageInstance"]//text()').extract()).strip()
            self.logger.debug('requestJson V2')
            requestData = json.loads(requestJson)
            primaryIdentityData = json.loads(primaryIdentityJson)

            return scrapy.Request(response.urljoin(requestData['request']),
                                  headers={'Csrf-Token': self.csrfToken,
                                           'X-Restli-Protocol-Version': '2.0.0',
                                           'X-Requested-With': 'XMLHttpRequest',
                                           'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8'},
                                  meta={'primaryIdentity': primaryIdentityData['primaryIdentity'],
                                        'clientPageInstance': clientPageInstance},
                                  callback=self.parseLeadsV2)

    def parseLeadsV2(self, response):
        try:
            data = json.loads(response.body)
        except:
            self.logger.error('Error in decoding accounts V2 json.')

        #self.logger.debug('parseLeadsV2 %s', response.text)
        if 'paging' in data:
            self.crawler.stats.set_value('total_items', data['paging']['total'])
            totalItemsShouldBeExtracted = data['paging']['total']
            print('Total paging: ', data['paging']['total'])
            if data['paging']['total'] > self.LINKEDIN_LEADS_LIMIT:
                totalItemsShouldBeExtracted = self.LINKEDIN_LEADS_LIMIT
            self.crawler.stats.set_value('total_items_should_be_extracted', totalItemsShouldBeExtracted)
            url = response.url
            if '&start=' not in url:
                url = url + '&start=' + str(data['paging']['start'])
            parsedURL = urlparse(url)
            urlQueryStringDict = parse_qs(parsedURL.query)
            start = data['paging']['start'] + data['paging']['count']

            if start > self.max_sales_leads:
                print('found max leads!')
                return

            if start < data['paging']['total']:
                url = url.replace('start=%s' % str(data['paging']['start']), 'start=%s' % str(start))
                yield scrapy.Request(url, headers={'Referer': response.url,
                                                   'Csrf-Token': self.csrfToken,
                                                   'X-Restli-Protocol-Version': '2.0.0',
                                                   'X-Requested-With': 'XMLHttpRequest',
                                                   'X-Li-Identity': response.meta['primaryIdentity'],
                                                   'X-Li-Page-Instance': response.meta['clientPageInstance'],
                                                   'X-Li-Lang': 'en_US'},
                                     meta=response.meta,
                                     callback=self.parseLeadsV2)
        if 'elements' in data:
            for lead in data['elements']:
                i = LinkedinLeadItem()
                i['publicIdentifier'] = data.get('publicIdentifier')
                i['spiderName'] = self.name
                i['extractEmailAddress'] = False  # TODO add extraction!
                memberId = str(lead['objectUrn'].replace('urn:li:member:', ''))
                i['memberId'] = memberId

                i['firstname'] = lead['firstName']
                i['lastname'] = lead['lastName']
                fullname = lead['fullName']
                i['fullname'] = fullname
                degree = -1
                if 'degree' in lead:
                    degree = lead['degree']
                    i['degree'] = degree
                i['canSendInMail'] = 0
                if 'premium' in lead:
                    i['canSendInMail'] = 1 if lead['premium'] else 0
                if 'geoRegion' in lead:
                    i['location'] = lead['geoRegion']
                i['inCrm'] = 0
                if 'crmStatus' in lead and 'imported' in lead['crmStatus']:
                    i['inCrm'] = 1 if lead['crmStatus']['imported'] else 0
                leadTags = []
                if 'tags' in lead:
                    for leadTagId in lead['tags']:
                        leadTags.append(self.sales_tags[leadTagId])

                i['tags'] = ('\n').join(leadTags)
                entityUrn = None
                if 'entityUrn' in lead:
                    entityUrn = ('').join(re.findall('urn:li:fs_salesProfile:\\((.+?)\\)', lead['entityUrn']))
                    if entityUrn:
                        i['profileLinkSN'] = 'https://www.linkedin.com/sales/people/%s' % entityUrn
                        entityUrns = entityUrn.split(',')
                        if len(entityUrns) == 3:
                            i['profileLink'] = 'https://www.linkedin.com/profile/view/?id=%s' % entityUrns[0]
                            i['entityUrn'] = entityUrns[0]
                if 'currentPositions' in lead:
                    for position in lead['currentPositions']:
                        if position['current'] == True:
                            if 'companyName' in position:
                                i['companyName'] = position['companyName']
                            if 'title' in position:
                                i['position'] = position['title']
                            if 'companyUrn' in position:
                                companyId = str(position['companyUrn'].replace('urn:li:fs_salesCompany:', ''))
                                i['companyId'] = companyId
                            break

                if 'profilePictureDisplayImage' in lead:
                    for image in lead['profilePictureDisplayImage']['artifacts']:
                        if image['width'] == 400:
                            i['picture'] = '%s%s' % (lead['profilePictureDisplayImage']['rootUrl'], image['fileIdentifyingUrlPathSegment'])
                            break

                # self.leads.append(i.__dict__['_values'])
                yield self.getPositionEducationForLeadsRequest(i)


    def getPositionEducationForLeadsRequest(self, item):
        if 'entityUrn' in item and item['entityUrn']:
            return scrapy.Request(self.profile_view_url % item['entityUrn'], headers={'Csrf-Token': self.csrfToken,
                                                                                      'X-Li-Track': '{"clientVersion":"1.0.*","osName":"web","timezoneOffset":1,"deviceFormFactor":"DESKTOP","mpName":"voyager-web"}',
                                                                                      'X-Restli-Protocol-Version': '2.0.0',
                                                                                      'X-Requested-With': 'XMLHttpRequest',
                                                                                      'Referer': 'https://www.linkedin.com/feed'}, meta={'item': item},
                                  callback=self.parsePositionEducationForLeads)

    def parsePositionEducationForLeads(self, response):
        try:
            data = json.loads(response.body)
        except:
            self.logger.error('Error in decoding json (parsePositionEducationForLeads).')

        #self.logger.debug('parsePositionEducationForLeads %s' % response.body.decode('utf8', 'ignore'))
        i = response.meta['item']
        #self.logger.debug('parsePositionEducationForLeads %s', response.body_as_unicode())

        if 'educationView' in data and 'elements' in data['educationView']:
            educations = []
            for element in data['educationView']['elements']:
                educations.append('%s at %s from %s to %s' % (
                    element['degreeName'] if 'degreeName' in element else 'N/A',
                    element['schoolName'] if 'schoolName' in element else 'N/A',
                    element['timePeriod']['startDate']['year'] if 'timePeriod' in element and 'startDate' in element['timePeriod'] else 'N/A',
                    element['timePeriod']['endDate']['year'] if 'timePeriod' in element and 'endDate' in element['timePeriod'] else 'N/A'))

            i['educations'] = ('\n').join(educations)

        if 'profile' in data and 'headline' in data['profile']:
            i['headline'] = data['profile']['headline']

            if 'publicIdentifier' in data['profile']['miniProfile']:
                i['publicIdentifier'] = data['profile']['miniProfile']['publicIdentifier']

            if 'miniProfile' in data['profile'] and 'publicIdentifier' in data['profile']['miniProfile']:
                i['profileLink'] = 'https://www.linkedin.com/in/%s' % data['profile']['miniProfile']['publicIdentifier']

        if 'languageView' in data and 'elements' in data['languageView']:
            languages = []
            for element in data['languageView']['elements']:
                if 'name' in element and element['name']:
                    languages.append(element['name'])

            i['languages'] = ('\n').join(languages)

        self.leads.append(i.__dict__['_values'])

    def closed(self, reason):
        if self.leads:
            print('Found %s leads' % len(self.leads), reason)
            all_keys = {k for d in self.leads for k in d.keys()}

            with open(self.settings.get('OUT_FILE', f'unknown_{datetime.utcnow()}'), 'w') as result_file:
                dict_writer = csv.DictWriter(result_file, all_keys, dialect='excel')
                dict_writer.writeheader()
                dict_writer.writerows(self.leads)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Process some integers.')
    parser.add_argument('search_url', type=str)
    parser.add_argument('outfile', type=str)
    args = parser.parse_args()

    # results = []
    # def crawler_results(signal, sender, item, response, spider):
    #     results.append(item)
    #
    # dispatcher.connect(crawler_results, signal=signals.item_scraped)

    settings = {
        'PROXY_URL': 'https://45.77.91.16:33847',
        'PROXY_USERNAME': 'fqLMEg',
        'PROXY_PASSWORD': 'D6VQ4Y',
        'LINKEDIN_SESSION_KEY': 'oh9215345@gmail.com',
        'LINKEDIN_SESSION_PASSWORD': 'Pokemon151',
        'SEARCH_QUERY_URL': args.search_url,
        'OUT_FILE': args.outfile
    }

    process = CrawlerProcess(settings)
    process.crawl(LoginSpider)
    process.start()  # the script will block here until the crawling is finished
